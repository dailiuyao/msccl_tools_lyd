
c197-081:29701:29701 [0] misc/cudawrap.cc:112 NCCL WARN cuDriverGetVersion failed with 34
c197-081:29701:29701 [0] NCCL INFO Bootstrap : Using ib0:192.168.44.137<0>
c197-081:29701:29701 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
c197-081:29701:29746 [0] NCCL INFO NET/IB : Using [0]mlx4_0:1/IB [RO]; OOB ib0:192.168.44.137<0>
c197-081:29701:29746 [0] NCCL INFO Using network IB
c197-081:29701:29746 [0] NCCL INFO Setting affinity for GPU 0 to 0f000f
c197-081:29701:29746 [0] NCCL INFO NCCL_MAX_NCHANNELS set by environment to 1.
c197-081:29701:29746 [0] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 1.
c197-081:29701:29746 [0] NCCL INFO Trees [0] 9/12/-1->8->0
c197-081:29701:29746 [0] NCCL INFO Channel 00/0 : 7[83000] -> 8[2000] [receive] via NET/IB/0
c197-081:29701:29746 [0] NCCL INFO Channel 00 : 8[2000] -> 9[3000] via SHM/direct/direct
c197-081:29701:29746 [0] NCCL INFO Connected all rings
c197-081:29701:29746 [0] NCCL INFO Channel 00/0 : 8[2000] -> 12[2000] [send] via NET/IB/0
c197-081:29701:29746 [0] NCCL INFO Channel 00/0 : 0[2000] -> 8[2000] [receive] via NET/IB/0
c197-081:29701:29746 [0] NCCL INFO Channel 00/0 : 8[2000] -> 0[2000] [send] via NET/IB/0
c197-081:29701:29746 [0] NCCL INFO Channel 00/0 : 12[2000] -> 8[2000] [receive] via NET/IB/0
c197-081:29701:29746 [0] NCCL INFO Connected all trees
c197-081:29701:29746 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
c197-081:29701:29746 [0] NCCL INFO 1 coll channels, 1 p2p channels, 1 p2p channels per peer
c197-081:29701:29746 [0] NCCL INFO comm 0x592c430 rank 8 nranks 16 cudaDev 0 busId 2000 - Init COMPLETE
HOST | common.cu:1021 | run() | ncclGroupEnd() finished
HOST | common.cu:1033 | run() | cudaHostAlloc() finished
HOST | common.cu:645 | threadRunTests() | ncclTestEngine().runTest start
HOST | allreduce.cu:88 | AllReduceRunTest() | TimeTest start
HOST | allreduce.cu:91 | AllReduceRunTest() | start type_count 0, op_count 0
HOST | common.cu:604 | TimeTest() | before startColl for large size
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:610 | TimeTest() | after completeColl for large size
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:619 | TimeTest() | before BenchTime()
HOST | common.cu:627 | TimeTest() | The first BenchTime launched, start out of place
HOST | common.cu:419 | BenchTime() | start syc
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 0, iter is 0
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:423 | BenchTime() | end sync
HOST | common.cu:443 | BenchTime() | before Performance Benchmark
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 0, iter is 0
HOST | common.cu:452 | BenchTime() | after Performance Benchmark
HOST | common.cu:455 | BenchTime() | start graph capture
HOST | common.cu:474 | BenchTime() | end graph capture
HOST | common.cu:477 | BenchTime() | before Performance Benchmark's completeColl()
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:481 | BenchTime() | after Performance Benchmark's completeColl()
HOST | common.cu:501 | BenchTime() | after args->collTest->getBw and before barrier()
HOST | common.cu:505 | BenchTime() | after args->collTest->getBw and barrier()

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 0, iter is 0
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384
HOST | common.cu:629 | TimeTest() | The first BenchTime finished, end out of place
HOST | common.cu:630 | TimeTest() | The second BenchTime launched, start in place
HOST | common.cu:419 | BenchTime() | start syc
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 1, iter is 0
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:423 | BenchTime() | end sync
HOST | common.cu:443 | BenchTime() | before Performance Benchmark
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 1, iter is 0
HOST | common.cu:452 | BenchTime() | after Performance Benchmark
HOST | common.cu:455 | BenchTime() | start graph capture
HOST | common.cu:474 | BenchTime() | end graph capture
HOST | common.cu:477 | BenchTime() | before Performance Benchmark's completeColl()
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:481 | BenchTime() | after Performance Benchmark's completeColl()
HOST | common.cu:501 | BenchTime() | after args->collTest->getBw and before barrier()
HOST | common.cu:505 | BenchTime() | after args->collTest->getBw and barrier()

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 1, iter is 0
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384
HOST | common.cu:632 | TimeTest() | The second BenchTime finished, end in place
HOST | common.cu:636 | TimeTest() | after BenchTime()
HOST | allreduce.cu:93 | AllReduceRunTest() | end type_count 0, op_count 0
HOST | allreduce.cu:96 | AllReduceRunTest() | TimeTest finished
HOST | common.cu:647 | threadRunTests() | ncclTestEngine().runTest finished
c197-081:29701:29701 [0] NCCL INFO comm 0x592c430 rank 8 nranks 16 cudaDev 0 busId 2000 - Destroy COMPLETE
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 654.812073 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.353882 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.541870 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.529785 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.551086 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.524841 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.533325 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.528381 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.535461 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.529053 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.528381 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.524109 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.495056 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.519836 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.511353 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.522705 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.523376 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.531921 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.514893 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 843.483704 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.376587 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.371643 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.384399 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.401428 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.350342 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.395752 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.392212 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.399292 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.423401 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.387939 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.412048 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.404968 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.433319 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.421997 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.382263 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.433319 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.443970 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.390076 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.441132 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 388.390778 us
