# nThread 1 nGpus 1 minBytes 1048576 maxBytes 1048576 step: 2(factor) warmup iters: 0 iters: 1 agg iters: 1 validation: 1 graph: 0
#
# Using devices
#  Rank  0 Group  0 Pid  10983 on   c197-071 device  0 [0x02] Quadro RTX 5000
#  Rank  1 Group  0 Pid  10984 on   c197-071 device  1 [0x03] Quadro RTX 5000
#  Rank  2 Group  0 Pid  10985 on   c197-071 device  2 [0x82] Quadro RTX 5000
#  Rank  3 Group  0 Pid  10986 on   c197-071 device  3 [0x83] Quadro RTX 5000
#  Rank  4 Group  0 Pid   9230 on   c197-072 device  0 [0x02] Quadro RTX 5000
#  Rank  5 Group  0 Pid   9231 on   c197-072 device  1 [0x03] Quadro RTX 5000
#  Rank  6 Group  0 Pid   9232 on   c197-072 device  2 [0x82] Quadro RTX 5000
#  Rank  7 Group  0 Pid   9233 on   c197-072 device  3 [0x83] Quadro RTX 5000
#  Rank  8 Group  0 Pid  29701 on   c197-081 device  0 [0x02] Quadro RTX 5000
#  Rank  9 Group  0 Pid  29702 on   c197-081 device  1 [0x03] Quadro RTX 5000
#  Rank 10 Group  0 Pid  29703 on   c197-081 device  2 [0x82] Quadro RTX 5000
#  Rank 11 Group  0 Pid  29704 on   c197-081 device  3 [0x83] Quadro RTX 5000
#  Rank 12 Group  0 Pid   5063 on   c197-082 device  0 [0x02] Quadro RTX 5000
#  Rank 13 Group  0 Pid   5064 on   c197-082 device  1 [0x03] Quadro RTX 5000
#  Rank 14 Group  0 Pid   5065 on   c197-082 device  2 [0x82] Quadro RTX 5000
#  Rank 15 Group  0 Pid   5066 on   c197-082 device  3 [0x83] Quadro RTX 5000
c197-071:10983:10983 [0] NCCL INFO Bootstrap : Using ib0:192.168.44.135<0>
c197-071:10983:10983 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

c197-071:10983:10983 [0] misc/cudawrap.cc:112 NCCL WARN cuDriverGetVersion failed with 34
NCCL version 2.14.3+cuda11.3
c197-071:10983:11024 [0] NCCL INFO NET/IB : Using [0]mlx4_0:1/IB [RO]; OOB ib0:192.168.44.135<0>
c197-071:10983:11024 [0] NCCL INFO Using network IB
c197-071:10983:11024 [0] NCCL INFO Setting affinity for GPU 0 to 0f000f
c197-071:10983:11024 [0] NCCL INFO NCCL_MAX_NCHANNELS set by environment to 1.
c197-071:10983:11024 [0] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 1.
c197-071:10983:11024 [0] NCCL INFO Channel 00/01 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
c197-071:10983:11024 [0] NCCL INFO Trees [0] 1/8/-1->0->-1
c197-071:10983:11024 [0] NCCL INFO Channel 00/0 : 15[83000] -> 0[2000] [receive] via NET/IB/0
c197-071:10983:11024 [0] NCCL INFO Channel 00 : 0[2000] -> 1[3000] via SHM/direct/direct
c197-071:10983:11024 [0] NCCL INFO Connected all rings
c197-071:10983:11024 [0] NCCL INFO Channel 00/0 : 8[2000] -> 0[2000] [receive] via NET/IB/0
c197-071:10983:11024 [0] NCCL INFO Channel 00/0 : 0[2000] -> 8[2000] [send] via NET/IB/0
c197-071:10983:11024 [0] NCCL INFO Connected all trees
c197-071:10983:11024 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
c197-071:10983:11024 [0] NCCL INFO 1 coll channels, 1 p2p channels, 1 p2p channels per peer
c197-071:10983:11024 [0] NCCL INFO comm 0x5eb39c0 rank 0 nranks 16 cudaDev 0 busId 2000 - Init COMPLETE
HOST | common.cu:1021 | run() | ncclGroupEnd() finished
HOST | common.cu:1033 | run() | cudaHostAlloc() finished
#
#                                                              out-of-place                       in-place          
#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong
#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       
HOST | common.cu:645 | threadRunTests() | ncclTestEngine().runTest start
HOST | allreduce.cu:88 | AllReduceRunTest() | TimeTest start
HOST | allreduce.cu:91 | AllReduceRunTest() | start type_count 0, op_count 0
HOST | common.cu:604 | TimeTest() | before startColl for large size
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:610 | TimeTest() | after completeColl for large size
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:619 | TimeTest() | before BenchTime()
     1048576        262144     float     sum      -1HOST | common.cu:627 | TimeTest() | The first BenchTime launched, start out of place
HOST | common.cu:419 | BenchTime() | start syc
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 0, iter is 0
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:423 | BenchTime() | end sync
HOST | common.cu:443 | BenchTime() | before Performance Benchmark
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 0, iter is 0
HOST | common.cu:452 | BenchTime() | after Performance Benchmark
HOST | common.cu:455 | BenchTime() | start graph capture
HOST | common.cu:474 | BenchTime() | end graph capture
HOST | common.cu:477 | BenchTime() | before Performance Benchmark's completeColl()
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:481 | BenchTime() | after Performance Benchmark's completeColl()
HOST | common.cu:501 | BenchTime() | after args->collTest->getBw and before barrier()
HOST | common.cu:505 | BenchTime() | after args->collTest->getBw and barrier()

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 0, iter is 0
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384
   1018.8    1.03    1.93      0HOST | common.cu:629 | TimeTest() | The first BenchTime finished, end out of place
HOST | common.cu:630 | TimeTest() | The second BenchTime launched, start in place
HOST | common.cu:419 | BenchTime() | start syc
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 1, iter is 0
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:423 | BenchTime() | end sync
HOST | common.cu:443 | BenchTime() | before Performance Benchmark
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 1, iter is 0
HOST | common.cu:452 | BenchTime() | after Performance Benchmark
HOST | common.cu:455 | BenchTime() | start graph capture
HOST | common.cu:474 | BenchTime() | end graph capture
HOST | common.cu:477 | BenchTime() | before Performance Benchmark's completeColl()
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()
HOST | common.cu:481 | BenchTime() | after Performance Benchmark's completeColl()
HOST | common.cu:501 | BenchTime() | after args->collTest->getBw and before barrier()
HOST | common.cu:505 | BenchTime() | after args->collTest->getBw and barrier()

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384
HOST | common.cu:332 | startColl() | start startColl(), type is 7, opIndex is 0, root is -1, in_place is 1, iter is 0
HOST | common.cu:402 | completeColl() | blocking_coll is: 0
HOST | common.cu:405 | completeColl() | before testStreamSynchronize()
HOST | common.cu:407 | completeColl() | after testStreamSynchronize()

 DEVICE | total size in run tree() is: 262144

 DEVICE | nthreads is: 640

 DEVICE | loopSize is: 16384

 DEVICE | nChannels is: 1

 DEVICE | chunkSize is: 16384
   1009.2    1.04    1.95      0HOST | common.cu:632 | TimeTest() | The second BenchTime finished, end in place

HOST | common.cu:636 | TimeTest() | after BenchTime()
HOST | allreduce.cu:93 | AllReduceRunTest() | end type_count 0, op_count 0
HOST | allreduce.cu:96 | AllReduceRunTest() | TimeTest finished
HOST | common.cu:647 | threadRunTests() | ncclTestEngine().runTest finished
c197-071:10983:10983 [0] NCCL INFO comm 0x5eb39c0 rank 0 nranks 16 cudaDev 0 busId 2000 - Destroy COMPLETE
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 635.534058 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.180847 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.097900 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.183716 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.172363 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.076599 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.087219 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.179443 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.170898 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.127686 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.119141 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.102844 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.147522 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.073730 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.159546 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.131226 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.162415 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.168823 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.163818 us
DEVICE | allreduce.h| runTreeUpDown | recvReduceCopy | time: 830.165955 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.811340 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.871628 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.823410 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.797867 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.834747 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.837585 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.824829 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.800705 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.815598 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.843979 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.853195 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.800705 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.844681 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.833328 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.851059 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.812057 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.876602 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.873047 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.879425 us
DEVICE | allreduce.h| runTreeUpDown | directSendFromOutput | time: 190.902832 us
# Out of bounds values : 0 OK
# Avg bus bandwidth    : 1.93901 
#

